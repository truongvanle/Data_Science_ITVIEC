# preprocessed.py

import pandas as pd
import regex
from underthesea import sentiment, word_tokenize
from nltk import sent_tokenize
from nltk.corpus import stopwords
import emoji
import nltk

# Khá»Ÿi táº¡o danh sÃ¡ch tá»« tiÃªu cá»±c, tÃ­ch cá»±c
positive_words = [
    "thÃ­ch", "tá»‘t", "xuáº¥t sáº¯c", "tuyá»‡t vá»i", "á»•n","hÃ i lÃ²ng", "Æ°ng Ã½", "hoÃ n háº£o", "cháº¥t lÆ°á»£ng", "nhanh",
    "thÃ¢n thiá»‡n", "ráº¥t tá»‘t", "ráº¥t thÃ­ch", "táº­n tÃ¢m", "Ä‘Ã¡ng tin cáº­y", "Ä‘áº³ng cáº¥p","an tÃ¢m", "thÃºc Ä‘áº©y", "cáº£m Ä‘á»™ng", 
    "ná»•i trá»™i","sÃ¡ng táº¡o", "phÃ¹ há»£p", "hiáº¿m cÃ³", "cáº£i thiá»‡n", "hoÃ  nhÃ£", "chÄƒm chá»‰", "cáº©n tháº­n",
    "vui váº»", "hÃ o há»©ng", "Ä‘am mÃª", 'chuyÃªn', 'dá»…', 'giá»i', 'hay', 'hiá»‡u', 'hÃ i', 'há»— trá»£', 'nhiá»‡t tÃ¬nh',
    'thÃ¢n', 'tuyá»‡t', 'vui', 'chuyÃªn nghiá»‡p', 'Ä‘á»™ng lá»±c', 'dá»… chá»‹u','cÃ´ng báº±ng', 'háº¡nh phÃºc', 'há»£p lÃ½','truyá»n cáº£m há»©ng', 
    'phÃ¡t triá»ƒn', 'ná»•i báº­t','há»£p tÃ¡c', 'Ä‘á»“ng Ä‘á»™i', 'hÃ²a Ä‘á»“ng', 'há»c há»i', 'tÃ´n trá»ng', 'tá»‘t nháº¥t', 'vui má»«ng', 'Ä‘áº³ng cáº¥p',
    'dá»… dÃ ng', 'chá»§ Ä‘á»™ng', 'Ä‘á»“ng cáº£m', 'cáº£m', 'má»Ÿ rá»™ng', 'bÃ¬nh Ä‘áº³ng', 'nÄƒng Ä‘á»™ng', 'thoáº£i mÃ¡i', 'máº¿n', 'cáº£m Æ¡n', 'tá»‘t hÆ¡n','cá»Ÿi má»Ÿ', 'cÆ¡ há»™i'
]
negative_words = [
    "kÃ©m", "tá»‡", "buá»“n", "khÃ´ng dá»… chá»‹u", "khÃ´ng thÃ­ch", "khÃ´ng á»•n", "Ã¡p lá»±c", "má»‡t","khÃ´ng Ä‘Ã¡ng tin cáº­y", "khÃ´ng chuyÃªn nghiá»‡p",
    "khÃ´ng thÃ¢n thiá»‡n", "khÃ´ng tá»‘t", "cháº­m", "khÃ³ khÄƒn", "phá»©c táº¡p",
    "khÃ³ chá»‹u", "gÃ¢y khÃ³ dá»…", "rÆ°á»m rÃ ", "tá»“i tá»‡", "khÃ³ xá»­", "khÃ´ng thá»ƒ cháº¥p nháº­n", "khÃ´ng rÃµ rÃ ng",
    "rá»‘i ráº¯m", 'khÃ´ng hÃ i lÃ²ng', 'quÃ¡ tá»‡', 'ráº¥t tá»‡', "phiá»n phá»©c",
    'tháº¥t vá»ng', 'tá»‡ háº¡i', 'kinh khá»§ng', 'chÃ¡n', 'drama', 'dramas', 'gÃ¡p', 'gáº¯t',
    'lá»—i', 'ngáº¯t', 'quÃ¡i', 'quÃ¡t', 'thiáº¿u', 'trá»…', 'tá»‡p', 'tá»“i', "hÃ¡ch dá»‹ch",
    'cÄƒng tháº³ng', 'khÃ´ng hÃ²a Ä‘á»“ng', 'thiáº¿u Ä‘Ã o táº¡o', 'thiáº¿u sÃ¡ng táº¡o',
    'khá»§ng hoáº£ng', 'rá»‘i loáº¡n', 'khÃ´ng cÃ³ cÆ¡ há»™i', 'thiáº¿u cÃ´ng báº±ng', 'khÃ´ng cháº¥p nháº­n Ä‘Æ°á»£c',
    'khÃ´ng Ä‘á»§', 'thiáº¿u sá»± cÃ´ng nháº­n', 'thiáº¿u há»— trá»£', 'khÃ´ng há»£p', 'thiáº¿u cÆ¡ há»™i thÄƒng tiáº¿n', 'Ã¡p', 'trÃ¬ trá»‡', 'tháº¥t báº¡i',
    'thiáº¿u sá»± minh báº¡ch', 'buá»“n bÃ£', 'rá»‘i', 'khÃ´ng Ä‘Ã¡ng', 'mÃ¢u thuáº«n',
    'thiáº¿u chuyÃªn nghiá»‡p', 'thiáº¿u Ä‘á»™ng lá»±c', 'lo láº¯ng', 'mÃ´i trÆ°á»ng thiáº¿u cá»Ÿi má»Ÿ', 'má»‡t má»i','lo'
    'thiáº¿u linh hoáº¡t', 'khÃ´ng tÃ´n trá»ng', 'tá»©c giáº­n', 'khÃ´ng phÃ¡t triá»ƒn', 'thiáº¿u sá»± rÃµ rÃ ng', 'bá»±c bá»™i'
]

features =[
    'Salary & benefits',
    'Training & learning',
    'Management cares about me',
    'Culture & fun',
    'Office & workspace'
]


# Danh sÃ¡ch tá»« dá»«ng tiáº¿ng Viá»‡t
vietnamese_stopwords = [
    "lÃ ", "cá»§a", "vÃ ", "cÃ³", "Ä‘Æ°á»£c", "trong", "cho", "vá»›i", "táº¡i", "bá»Ÿi",
    "Ä‘á»ƒ", "mÃ ", "nÃ y", "kia", "nÃ³", "há»", "nhÆ°", "thÃ¬", "Ä‘Ã£", "Ä‘ang",
    "rá»“i", "má»™t", "nhá»¯ng", "cÃ¡c", "tá»«ng", "cÅ©ng", "ra", "vÃ o", "trÃªn",
    "dÆ°á»›i", "gÃ¬", "khi", "nÃ o", "Ä‘Ã¢u", "ai", "báº±ng", "theo", "vá»"
]

# Tá»« Ä‘iá»ƒn emoji
emoji_dict = {
    "ğŸ˜Š": "vui",
    "ğŸ˜¢": "buá»“n",
    "ğŸ‘": "tÃ­ch cá»±c",
    "ğŸ‘": "tiÃªu cá»±c"
}

# Tá»« Ä‘iá»ƒn teencode
teen_dict = {
    "k": "khÃ´ng",
    "ko": "khÃ´ng",
    "ng": "ngÆ°á»i",
    "nt": "nháº¯n tin",
    "ok": "á»•n",
    "oke": "á»•n"
}

# Danh sÃ¡ch tá»« sai
wrong_lst = ["", " ", "\n"]

def process_text(text):
    """
    Tiá»n xá»­ lÃ½ vÄƒn báº£n Ä‘á»ƒ chuáº©n bá»‹ cho phÃ¢n tÃ­ch cáº£m xÃºc

    Parameters:
        text (str): Ä‘oáº¡n vÄƒn báº£n Ä‘áº§u vÃ o

    Returns:
        str: vÄƒn báº£n Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½
    """
    # Chuyá»ƒn thÃ nh chá»¯ thÆ°á»ng vÃ  loáº¡i bá» kÃ½ tá»± khÃ´ng cáº§n thiáº¿t
    document = text.lower()
    document = document.replace("â€™", '')
    document = regex.sub(r'\.+', ".", document)

    new_sentence = ''
    for sentence in sent_tokenize(document):
        # Chuyá»ƒn emoji thÃ nh tá»«
        sentence = ''.join(emoji_dict.get(word, word) + ' ' for word in list(sentence))
        # Chuyá»ƒn teencode
        sentence = ' '.join(teen_dict.get(word, word) for word in sentence.split())
        # Loáº¡i bá» tá»« sai
        sentence = ' '.join('' if word in wrong_lst else word for word in sentence.split())
        # Loáº¡i bá» tá»« dá»«ng tiáº¿ng Anh vÃ  tiáº¿ng Viá»‡t
        stop_words = set(stopwords.words('english')) | set(vietnamese_stopwords)
        sentence = ' '.join(word for word in sentence.split() if word not in stop_words)
        new_sentence += sentence + '. '
    
    # Chuáº©n hÃ³a khoáº£ng tráº¯ng
    document = regex.sub(r'\s+', ' ', new_sentence).strip()
    return document

def count_pos_neg_words(text, pos_words=positive_words, neg_words=negative_words):
    """
    Äáº¿m sá»‘ cÃ¢u/cá»¥m tá»« tÃ­ch cá»±c vÃ  tiÃªu cá»±c trong vÄƒn báº£n sá»­ dá»¥ng underthesea vÃ  danh sÃ¡ch tá»« dá»± phÃ²ng

    Parameters:
        text (str): Ä‘oáº¡n vÄƒn báº£n Ä‘áº§u vÃ o
        pos_words (list): danh sÃ¡ch tá»«/cá»¥m tá»« tÃ­ch cá»±c
        neg_words (list): danh sÃ¡ch tá»«/cá»¥m tá»« tiÃªu cá»±c

    Returns:
        pos_count (int), neg_count (int): sá»‘ cÃ¢u/cá»¥m tá»« tÃ­ch cá»±c vÃ  tiÃªu cá»±c
    """
    # Tiá»n xá»­ lÃ½ vÄƒn báº£n
    text = process_text(text)
    
    # TÃ¡ch cÃ¢u hoáº·c cá»¥m tá»«
    sentences = [s.strip() for s in text.replace(".", ". ").replace(",", ", ").split(". ") if s.strip()]
    
    pos_count = 0
    neg_count = 0
    
    # PhÃ¢n tÃ­ch cáº£m xÃºc tá»«ng cÃ¢u/cá»¥m tá»« báº±ng underthesea
    for sentence in sentences:
        tokenized_sentence = " ".join(word_tokenize(sentence))
        result = sentiment(tokenized_sentence)
        if result == "positive":
            pos_count += 1
        elif result == "negative":
            neg_count += 1
        else:
            # Dá»± phÃ²ng: Kiá»ƒm tra danh sÃ¡ch tá»« náº¿u underthesea khÃ´ng nháº­n diá»‡n Ä‘Æ°á»£c
            tokens = tokenized_sentence.split()
            for i in range(len(tokens) - 1):
                phrase = tokens[i] + " " + tokens[i + 1]
                if phrase in pos_words:
                    pos_count += 1
                    tokens[i] = ""
                    tokens[i + 1] = ""
                elif phrase in neg_words:
                    neg_count += 1
                    tokens[i] = ""
                    tokens[i + 1] = ""
            # Äáº¿m tá»« Ä‘Æ¡n
            pos_count += sum(1 for token in tokens if token in pos_words and token)
            neg_count += sum(1 for token in tokens if token in neg_words and token)
    
    return pos_count, neg_count

def classify_sentiment(row):
    """
    GÃ¡n nhÃ£n cáº£m xÃºc dá»±a vÃ o Ä‘iá»ƒm Ä‘Ã¡nh giÃ¡ vÃ  sá»‘ cÃ¢u/cá»¥m tá»« cáº£m xÃºc

    Parameters:
        row (pd.Series): 1 dÃ²ng dá»¯ liá»‡u chá»©a Rating, word_count_positive, word_count_negative

    Returns:
        int: 0 = negative, 1 = neutral, 2 = positive
    """
    rating = row['Rating']
    pos_count = row['word_count_positive']
    neg_count = row['word_count_negative']
    
    if rating >= 4:
        return 2
    elif rating == 3:
        if pos_count > neg_count:
            return 2
        elif pos_count < neg_count:
            return 0
        else:
            return 1
    elif rating < 3:
        if rating == 0:
            if pos_count > neg_count:
                return 2
            elif pos_count < neg_count:
                return 0
            else:
                return 0
        else:
            return 0